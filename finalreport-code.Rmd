---
title: "DSE6211 Final Report - Code Notebook"
author: "Tracey Zicherman"
date: "2024-04-01"
output:
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
always_allow_html: true
---



```{r load_libraries, include=FALSE}

# load libraries
library(tidyverse)
library(plotly)
library(skimr)
library(tidymodels)
library(recipes)
library(reticulate)
library(keras3)
library(yardstick)
```


```{r setup_keras}
# Install Tensorflow backend - https://keras.posit.co/articles/getting_started.html - run once
# keras3::install_keras(backend = "tensorflow")
```

```{r setup_conflicts, include=FALSE}
# set preferences
tidymodels_prefer()
conflicted::conflicts_prefer(plotly::layout)
```

```{r setup_notebook}
# set notebook chunk defaults
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE
)
```

## Load Train and Test Data

```{r load_train_test}
train_data <- read_csv("train.csv")
test_data <- read_csv("test.csv")
```



## Recap: Build Baseline Model

```{r build_and_compile_mlp, include=FALSE}
# Separate predictors and response
x_train <- as.matrix(train_data %>% select(-booking_status))
y_train <- as.matrix(train_data$booking_status)

x_test <- as.matrix(test_data %>% select(-booking_status))
y_test <- as.matrix(test_data$booking_status)


# Define Sequential model with an explicit Input layer
inputs <- layer_input(shape = c(ncol(x_train)), name = "input_layer")
x <- inputs %>%
  layer_dense(units = 128, activation = "relu", name = "layer1") %>%
  layer_dense(units = 64, activation = "relu", name = "layer2") %>%
  layer_dense(units = 32, activation = "relu", name = "layer3") %>%
  layer_dense(units = 1, activation = 'sigmoid', name = "layer4")

model <- keras_model(inputs = inputs, outputs = x)

# Compile the model
model %>% compile(
  optimizer = optimizer_adam(),
  loss = loss_binary_crossentropy(),
  metrics = c('accuracy')
)

# Define early stopping callback
early_stopping <- callback_early_stopping(
  monitor = "val_loss",
  patience = 10, # Number of epochs with no improvement after which training will be stopped
  restore_best_weights = TRUE # Restore model weights from the epoch with the best value of the monitored quantity
)
```



##### Train Baseline Model

Different batch sizes and number of epochs were tested. A batch size of 128 seemed to give good results. We chose a 80-20 train-validation split. 

After testing the maximum number of training epochs at 100, the model seemed to be overfitting early, so we set an early stopping criterion, with a patience of 10 epochs. This stops the training if there is no improvement in the validation accuracy over 10 epochs. We left this patience number intentionally high to see where the model was overfitting.

A batch size of 32 seemed to give the best results. There wasn't much difference however, between batch sized of 32, 64, and 128

We also used a train-validation split of 80-20, smaller proportion of validation data gave jumpier accuracy and loss curves during training.

```{r train_mlp, include=FALSE}
# Fit the model with early stopping
history <- model %>% fit(
  x_train, y_train,
  batch_size = 32,
  epochs = 100,
  validation_split = 0.2,
  callbacks=list(early_stopping)) # Use 20% of the training data for validation
```


#### Evaluate Baseline Model

In this section we evaluate some of the results of training our baseline model.

The training and validation and accuracy loss curves show a smooth curve for the training data but a rather jumpy curve for the validation data. 

We note that the training accuracy and loss continue to improve the entire time but the early stopping stops the training around when the model seems to begin overfitting.

We may experiment with different architectures to see if we can improve these results.

```{r plot_history, fig.width=5, fig.height=3}
# Convert history to data frame
history_df <- data.frame(
  epoch = seq_len(length(history$metrics$loss)),
  loss = history$metrics$loss,
  val_loss = history$metrics$val_loss,
  accuracy = history$metrics$accuracy,
  val_accuracy = history$metrics$val_accuracy
)

# Plot training and validation loss
loss_plot <- ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = loss, color = "Training Loss")) +
  geom_line(aes(y = val_loss, color = "Validation Loss")) +
  labs(title = "Training and Validation Loss", x = "Epoch", y = "Loss") +
  scale_color_manual("", breaks = c("Training Loss", "Validation Loss"),
                     values = c("darkblue", "darkgreen")) +
  theme_minimal()

# Plot training and validation accuracy
accuracy_plot <- ggplot(history_df, aes(x = epoch)) +
  geom_line(aes(y = accuracy, color = "Training Accuracy")) +
  geom_line(aes(y = val_accuracy, color = "Validation Accuracy")) +
  labs(title = "Training and Validation Accuracy", x = "Epoch", y = "Accuracy") +
  scale_color_manual("", breaks = c("Training Accuracy", "Validation Accuracy"),
                     values = c("darkblue", "darkgreen")) +
  theme_minimal()

# Display the plots
print(loss_plot)
print(accuracy_plot)
```


```{r get_mlp_preds}

# Make predictions
pred_prob <- model %>% predict(x_test)
pred_class <- ifelse(pred_prob > 0.5, 1, 0)

# Create a tibble with actual and predicted values
results <- tibble(
  truth = as.factor(y_test),
  prediction = as.factor(pred_class)
)

# Calculate accuracy, precision, recall, and F1 score
accuracy <- results %>%
  metrics(truth, prediction) %>%
  filter(.metric == "accuracy")

precision <- results %>%
  precision(truth, prediction)

recall <- results %>%
  recall(truth, prediction)

f1_score <- results %>%
  f_meas(truth, prediction, beta = 1)

# Combine all metrics into a single tibble
metrics <- bind_rows(accuracy, precision, recall, f1_score)

```

Test accuracy is relatively close, though slightly lower than, validation accuracy, which is perhaps not surprising.


##

## 
